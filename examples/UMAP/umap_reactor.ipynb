{
 "cells": [
  {
   "source": [
    "# UMAP example on 2D-plane data from LES simulation\n",
    "\n",
    "Here, the UMAP dimensionality reduction algorithm will by used on a small dataset. This will be a heavily downsampled 2D plane containing instantaneous data from a 3D LES simulation of the oxidation reaction of NOx by O3 inside a turbulent jet in counterflow reactor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(10,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/LES/2D/2D_X_structured_subs5.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was created in Paraview by converting the original unstructured plane data into a structured grid. The filters used are *Resample to Image* followed by *Extract Subset*. To do so, Paraview inserted some \"ghost cells\" in which all variables are zero during the first filter. In the original unstructured dataset, these cells to not exists. This is done by Paraview as an easy way to resample to the desired structured grid. These cells can be indentified by the *vtkGhostType* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['vtkGhostType']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now eliminate the ghost cells (i.e. *vtkGhostType=2*)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data.vtkGhostType == 2].index, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the *vtkGhostType* column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['vtkGhostType'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply reset the indexes and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the pairplots look like (for a small subset of the data as the full dataset prompts the maximum allowed size error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data.iloc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some idea of what the data looks like by giving us all the 2D views of the data. But by reducing the dimension in a way that preserves as much of the structure of the data as possible we can get a visualisable representation of the data allowing us to \"see\" the data and its structure and begin to get some intuition about the data itself.\n",
    "\n",
    "To use UMAP for this task we need to first construct a UMAP object that will do the job for us. That is as simple as instantiating the class. So let's import the umap library and do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do any work with the data it will help to clean up it a little. It will be helpful to convert each feature into z-scores (number of standard deviations from the mean) for comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train our reducer, letting it learn about the manifold.\n",
    "For this UMAP follows the sklearn API and has a method ``fit`` which we\n",
    "pass the data we want the model to learn from. Since, at the end of the\n",
    "day, we are going to want to reduced representation of the data we will\n",
    "use, instead, the ``fit_transform`` method which first calls ``fit`` and\n",
    "then returns the transformed data as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.fit_transform(scaled_data)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with only two feature columns. This is because, by default, UMAP\n",
    "reduces down to 2D. Each row of the array is a 2-dimensional\n",
    "representation of the corresponding region. Thus we can plot the\n",
    "``embedding`` as a standard scatterplot and color by the target array\n",
    "(since it applies to the transformed data which is in the same order as\n",
    "the original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8, 6])\n",
    "cm = plt.cm.get_cmap('inferno')\n",
    "lfs = 18\n",
    "tfs = 16\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=data['Phi'], vmin=0, vmax=5,\n",
    "    )\n",
    "plt.xticks(fontsize=tfs)\n",
    "plt.yticks(fontsize=tfs)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "cbaxes = inset_axes(ax, width=\"5%\", height=\"60%\", loc='upper right', borderpad=1)\n",
    "cb = plt.colorbar(cax=cbaxes, orientation='vertical', ticks=[0, 1, 2, 3, 4, 5])\n",
    "cbaxes.yaxis.set_ticks_position('left')\n",
    "cb.ax.tick_params(labelsize=tfs)\n",
    "cb.set_label(r'$\\Phi$', rotation=180, size=lfs, labelpad=-50)      \n",
    "cb.ax.set_yticklabels(['0', '1', '2', '3', '4', '< 5'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a useful job of capturing the structure of the data, and as\n",
    "can be seen from the matrix of scatterplots this is relatively accurate.\n",
    "Of course we learned at least this much just from that matrix of\n",
    "scatterplots -- which we could do since we only had four different\n",
    "dimensions to analyse. If we had data with a larger number of dimensions\n",
    "the scatterplot matrix would quickly become unwieldy to plot, and far\n",
    "harder to interpret. So moving on from the Penguin dataset, let's consider\n",
    "the digits dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}